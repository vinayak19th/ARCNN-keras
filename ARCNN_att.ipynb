{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ARCNN_att.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1nObMCTzaCdEY5z-ZtAd5ZBU0W-39gG9s",
      "authorship_tag": "ABX9TyMMFxKvieZ8r2fItHhRrDRa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayak19th/ARCNN-keras/blob/main/ARCNN_att.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bifmEONrBdM"
      },
      "source": [
        "# ARCNN Attention Based\n",
        "![PyPI - Python Version](https://img.shields.io/pypi/pyversions/Tensorflow?&logo=Python&style=for-the-badge)\n",
        "![Tensorflow Version](https://img.shields.io/static/v1?label=Tensorflow&message=2.1%2B&color=ffcc00&logo=Tensorflow&logoColor=ffcc00&style=for-the-badge)\n",
        "![Docker Image Size](https://img.shields.io/static/v1?label=DockerImage&message=3.35GB&color=0066ff&logo=Docker&style=for-the-badge)\n",
        "\n",
        "Part of the [ARCNN-Keras](https://github.com/vinayak19th/ARCNN-keras) repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwu_cD88nkmJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Lambda, Conv2DTranspose, SeparableConv2D\n",
        "import glob\n",
        "import os"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaARzk_sq_GL"
      },
      "source": [
        "## Creating the Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCY-CuvoonT6"
      },
      "source": [
        "@tf.function\n",
        "def create_pairs(flist,jpq=(10,20)):\n",
        "    images = tf.TensorArray(tf.float16,dynamic_size=True,size=0,infer_shape=True)\n",
        "    images_comp = tf.TensorArray(tf.float16,dynamic_size=True,size=0,infer_shape=True)\n",
        "    c =0\n",
        "    for file in flist:\n",
        "        y = tf.image.decode_jpeg(tf.io.read_file(file))\n",
        "        x = tf.image.random_jpeg_quality(y,jpq[0],jpq[1])\n",
        "        y = tf.expand_dims(tf.image.rgb_to_yuv(tf.cast(y,tf.float16))[:,:,0],-1)/255\n",
        "        x = tf.expand_dims(tf.image.rgb_to_yuv(tf.cast(x,tf.float16))[:,:,0],-1)/255\n",
        "        images = images.write(c,y)\n",
        "        images_comp =images_comp.write(c,x)\n",
        "        c+=1\n",
        "    y = images.stack()\n",
        "    x = images_comp.stack()\n",
        "    return (x,y)\n",
        "\n",
        "@tf.function\n",
        "def create_patches(x,y,p,s):\n",
        "    print(\"Shapes\")\n",
        "    batch_size = tf.shape(y)[0]\n",
        "    print(batch_size)\n",
        "    #Extracting patches and converting into batches\n",
        "    y_patches = tf.image.extract_patches(images=y,sizes=(1,p,p,1),strides=(1,s,s,1),rates=(1,1,1,1),padding='VALID')\n",
        "    #Calculating patch sizes and batches\n",
        "    shapes= tf.shape(y_patches)\n",
        "    patch_batch = int(shapes[1]*shapes[2]*batch_size)\n",
        "    \n",
        "    y_patches = tf.reshape(y_patches,(patch_batch,p,p,1))\n",
        "    print(\"y_patches :\",y_patches.shape)\n",
        "    \n",
        "    x_patches = tf.image.extract_patches(images=x,sizes=(1,p,p,1),strides=(1,s,s,1),rates=(1,1,1,1),padding='VALID')\n",
        "    x_patches = tf.reshape(x_patches,(patch_batch,p,p,1))\n",
        "    print(\"x_patches :\",x_patches.shape)\n",
        "    return (x_patches,y_patches)\n",
        "\n",
        "def create_artifact_dataset(fpath = \"HarmonicI_720p_1000k_1440p_bicubic/480/\",batch_size=32,p=100,s=42,jpq=(10,20),fformat=\"*.jpg\"):\n",
        "    \"\"\"\n",
        "    Wrapper function to return tf.dataset object with all the data\n",
        "        fpath : Path to folder containing jpeg files\n",
        "            ex:HarmonicI_720p_1000k_1440p_bicubic/480/\n",
        "            HR should be a similar directory with the parent changed from 480 to 960\n",
        "            ex:HarmonicI_720p_1000k_1440p_bicubic/960/\n",
        "        batch_size : size of batches per batch of patches\n",
        "        p : Patch size\n",
        "        s : stride size\n",
        "        jpq : Tuple(min,max)\n",
        "            ex: jpq = (10,20) ; where min quality is 10 and max is 20\n",
        "    \"\"\"\n",
        "    flist = glob.glob(os.path.join(fpath,fformat))\n",
        "    print(\"flist:\",len(flist))\n",
        "    artifact_dataset = tf.data.Dataset.from_tensor_slices(flist).batch(32)\n",
        "    \n",
        "    func = lambda x: create_pairs(x,jpq)\n",
        "    artifact_dataset = artifact_dataset.map(func,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    print(\"JPEG Pairs created with quality of range:\",jpq,\"\\n--------------------\")\n",
        "    \n",
        "    func = lambda x,y: create_patches(x,y,p,s)\n",
        "    artifact_dataset = artifact_dataset.map(func,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    print(\"Created Patches\\n--------------------\")\n",
        "    \n",
        "    artifact_dataset = artifact_dataset.unbatch().batch(batch_size)\n",
        "    print(\"Dataset batches of batch size\",batch_size,\"\\n--------------------\")\n",
        "    print(\"Dataset Spec:\\n\",artifact_dataset.element_spec)\n",
        "    \n",
        "    artifact_dataset = artifact_dataset.cache()\n",
        "    return artifact_dataset"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpteHpK5qdU1",
        "outputId": "38775466-3b80-4347-c0f0-62adba347940"
      },
      "source": [
        "data = create_artifact_dataset(fpath=\"/content/drive/MyDrive/Colab_Notebooks/arcnn/480\")\n",
        "data = data.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "flist: 720\n",
            "JPEG Pairs created with quality of range: (10, 20) \n",
            "--------------------\n",
            "Shapes\n",
            "Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
            "y_patches : (None, 100, 100, 1)\n",
            "x_patches : (None, 100, 100, 1)\n",
            "Created Patches\n",
            "--------------------\n",
            "Dataset batches of batch size 32 \n",
            "--------------------\n",
            "Dataset Spec:\n",
            " (TensorSpec(shape=(None, 100, 100, 1), dtype=tf.float16, name=None), TensorSpec(shape=(None, 100, 100, 1), dtype=tf.float16, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeEr0F6Gsqnv"
      },
      "source": [
        "## Creating the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c0EmfuWs1PA"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWJ30vMwoQx_"
      },
      "source": [
        "class PixelAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, nf,name ='PixAttention'):\n",
        "        super(PixelAttention, self).__init__(name=name)\n",
        "        self._name = name\n",
        "        self.conv1 = Conv2D(filters=nf,kernel_size=1)\n",
        "    \n",
        "    def call(self,x):\n",
        "        y = self.conv1(x)\n",
        "        self.sig = tf.keras.activations.sigmoid(y)\n",
        "        out = tf.math.multiply(x,y)\n",
        "        return out"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5WfTbbdntJ5"
      },
      "source": [
        "def get_ARCNN_att(input_shape=(32,32,1)):\n",
        "    inp = Input(shape=input_shape)\n",
        "    conv1 = Conv2D(32,5,dilation_rate=4,activation='relu', padding='same', use_bias=True,name=\"Feature_extract\")(inp)\n",
        "    conv2 = Conv2D(32,5,dilation_rate=2,activation='relu', padding='same', use_bias=True,name=\"Feature_Enhance\")(conv1)\n",
        "    pa2 = PixelAttention(32,name=\"PA2\")(conv2)\n",
        "    conv3 = Conv2D(32,1,activation='relu', padding='valid', use_bias=True,name=\"Mapping\")(pa2)\n",
        "    pa3 = PixelAttention(32,name=\"PA3\")(conv3)\n",
        "    conv4 = Conv2D(1,3,dilation_rate=4,name=\"Image\",padding='same')(pa3)\n",
        "    ARCNN = Model(inputs=inp,outputs=conv4)\n",
        "    return ARCNN"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXtFhiytoiVs",
        "outputId": "b6068d3f-1e9d-4b58-c88d-b4412a1e2ff4"
      },
      "source": [
        "ARCNN = get_ARCNN_att([None,None,1])\n",
        "ARCNN.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, None, None, 1)]   0         \n",
            "_________________________________________________________________\n",
            "Feature_extract (Conv2D)     (None, None, None, 32)    832       \n",
            "_________________________________________________________________\n",
            "Feature_Enhance (Conv2D)     (None, None, None, 32)    25632     \n",
            "_________________________________________________________________\n",
            "PA2 (PixelAttention)         (None, None, None, 32)    1056      \n",
            "_________________________________________________________________\n",
            "Mapping (Conv2D)             (None, None, None, 32)    1056      \n",
            "_________________________________________________________________\n",
            "PA3 (PixelAttention)         (None, None, None, 32)    1056      \n",
            "_________________________________________________________________\n",
            "Image (Conv2D)               (None, None, None, 1)     289       \n",
            "=================================================================\n",
            "Total params: 29,921\n",
            "Trainable params: 29,921\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew1PHJDps_Yp"
      },
      "source": [
        "### Custom losses and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yZ1-7Eps_AJ"
      },
      "source": [
        "def ssim(y_true,y_pred):\n",
        "    return tf.image.ssim(y_true,y_pred,max_val=1.0)\n",
        "\n",
        "def psnr(y_true,y_pred):\n",
        "    return tf.image.psnr(y_true,y_pred,max_val=1.0)\n",
        "\n",
        "@tf.function\n",
        "def custom_loss(y_true, y_pred):\n",
        "    alpha = tf.constant(0.30)\n",
        "    mssim = alpha*(1-tf.image.ssim_multiscale(y_true,y_pred,max_val=1.0,filter_size=3))\n",
        "    mse = tf.metrics.mae(y_true, y_pred)\n",
        "    loss = tf.reduce_mean(mssim) + (1-alpha)*tf.reduce_mean(mse)\n",
        "    return loss\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaeOpxpds36I"
      },
      "source": [
        "### Checkpoints & Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj3GUYy7vF2b"
      },
      "source": [
        "def makedirs(path):\n",
        "    try:\n",
        "        os.mkdir(path)\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps01KIQrqe9b"
      },
      "source": [
        "filepath=\"/content/drive/MyDrive/Colab_Notebooks/arcnn/checkpoints/weights-improvement-{epoch:02d}-{ssim:.2f}.hdf5\"\n",
        "cp = tf.keras.callbacks.ModelCheckpoint(filepath,monitor=\"ssim\",verbose=1,save_weights_only=True)\n",
        "makedirs(\"/content/drive/MyDrive/Colab_Notebooks/arcnn/checkpoints/\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_WEe_CnslRQ",
        "outputId": "23261204-977a-4a39-a803-8d1102eac903"
      },
      "source": [
        "optim = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
        "ARCNN.compile(optimizer=optim,loss=custom_loss,metrics=[ssim,psnr])\n",
        "ARCNN.fit(data,epochs=10,callbacks=[cp])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4050/4050 [==============================] - 226s 56ms/step - loss: 0.0266 - ssim: 0.8903 - psnr: 32.1650\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/Colab_Notebooks/arcnn/checkpoints/weights-improvement-01-0.89.hdf5\n",
            "Epoch 2/10\n",
            "4050/4050 [==============================] - 224s 55ms/step - loss: 0.0251 - ssim: 0.8938 - psnr: 32.6482\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/MyDrive/Colab_Notebooks/arcnn/checkpoints/weights-improvement-02-0.90.hdf5\n",
            "Epoch 3/10\n",
            "4050/4050 [==============================] - 225s 55ms/step - loss: 0.0250 - ssim: 0.8955 - psnr: 32.6609\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/MyDrive/Colab_Notebooks/arcnn/checkpoints/weights-improvement-03-0.90.hdf5\n",
            "Epoch 4/10\n",
            "4050/4050 [==============================] - 225s 56ms/step - loss: 0.0246 - ssim: 0.8966 - psnr: 32.7682\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/MyDrive/Colab_Notebooks/arcnn/checkpoints/weights-improvement-04-0.90.hdf5\n",
            "Epoch 5/10\n",
            "4050/4050 [==============================] - 225s 56ms/step - loss: 0.0242 - ssim: 0.8976 - psnr: 32.8856\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/MyDrive/Colab_Notebooks/arcnn/checkpoints/weights-improvement-05-0.90.hdf5\n",
            "Epoch 6/10\n",
            "4050/4050 [==============================] - 224s 55ms/step - loss: 0.0242 - ssim: 0.8983 - psnr: 32.8753\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/MyDrive/Colab_Notebooks/arcnn/checkpoints/weights-improvement-06-0.90.hdf5\n",
            "Epoch 7/10\n",
            "4050/4050 [==============================] - 225s 56ms/step - loss: 0.0238 - ssim: 0.8990 - psnr: 33.0030\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/MyDrive/Colab_Notebooks/arcnn/checkpoints/weights-improvement-07-0.90.hdf5\n",
            "Epoch 8/10\n",
            "4050/4050 [==============================] - 226s 56ms/step - loss: 0.0238 - ssim: 0.8993 - psnr: 32.9884\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/MyDrive/Colab_Notebooks/arcnn/checkpoints/weights-improvement-08-0.90.hdf5\n",
            "Epoch 9/10\n",
            "4050/4050 [==============================] - 226s 56ms/step - loss: 0.0235 - ssim: 0.9000 - psnr: 33.1227\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/MyDrive/Colab_Notebooks/arcnn/checkpoints/weights-improvement-09-0.90.hdf5\n",
            "Epoch 10/10\n",
            "4050/4050 [==============================] - 224s 55ms/step - loss: 0.0235 - ssim: 0.9001 - psnr: 33.1365\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/MyDrive/Colab_Notebooks/arcnn/checkpoints/weights-improvement-10-0.90.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2494ab3990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2Q_1xCvs9E_",
        "outputId": "26cdb054-a996-4824-f5bd-f98a1b7863e0"
      },
      "source": [
        "print(\"Saving Model\")\n",
        "ARCNN.save(\"/content/drive/MyDrive/Colab_Notebooks/arcnn/att\",save_format=\"tf\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving Model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_2_layer_call_fn, conv2d_2_layer_call_and_return_conditional_losses, conv2d_3_layer_call_fn, conv2d_3_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv2d_2_layer_call_fn, conv2d_2_layer_call_and_return_conditional_losses, conv2d_3_layer_call_fn, conv2d_3_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab_Notebooks/arcnn/att/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab_Notebooks/arcnn/att/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAy050HR-rgV"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAJNFNoX_aAi"
      },
      "source": [
        "import PIL"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcyb7aEB9bdf",
        "outputId": "2dee5681-1fc7-4358-95c1-50227ed54367"
      },
      "source": [
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab_Notebooks/arcnn/att\",compile=False)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHrGNzIX96Cz",
        "outputId": "830517cc-bfed-483b-aedb-0fd558267644"
      },
      "source": [
        "flist = np.asarray(glob.glob(os.path.join(\"/content/drive/MyDrive/Colab_Notebooks/arcnn/tests/\",\"*\")))\n",
        "count = 0\n",
        "total = len(flist) \n",
        "print(\"Processing\",total,\"files\")\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing 3 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7lljKBs-0vi"
      },
      "source": [
        "def process_image_SR(impath):\n",
        "  im = PIL.Image.open(impath)\n",
        "  im = im.convert('YCbCr') # For single channel inference\n",
        "  im = np.asanyarray(im)\n",
        "  y = np.expand_dims(im[:,:,0],-1)/255 # Normalizing input\n",
        "  uv = np.asanyarray(im)[:,:,1:]\n",
        "  #print(\"uv:\",uv.shape,\"| y:\",y.shape)\n",
        "  return (y,uv)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cecwFsvt-737",
        "outputId": "a52d59ba-f6f0-4b94-820b-96958aa16edc"
      },
      "source": [
        "prog = tf.keras.utils.Progbar(total,unit_name='frames')\n",
        "for i in flist:\n",
        "  im_y,im_uv = process_image_SR(i)\n",
        "  #print(im_y.shape)\n",
        "  im_y = np.expand_dims(im_y,0)\n",
        "  outs = ARCNN.predict(im_y)\n",
        "  count += 1\n",
        "  out = outs.reshape(im_y.shape[1], im_y.shape[2]) #Removing batch dimensions\n",
        "  y_pred = np.stack([out*255,im_uv[:,:,0],im_uv[:,:,1]],axis=-1)\n",
        "  y_pred= np.clip(y_pred,0,255).astype('uint8')\n",
        "  y_pred = PIL.Image.fromarray(y_pred,mode='YCbCr').convert('RGB')\n",
        "  fname = \"out\"+ i.split(\"/\")[-1]\n",
        "  converter = PIL.ImageEnhance.Color(y_pred)\n",
        "  y_pred = converter.enhance(1.4)\n",
        "  y_pred.save(\"/content/drive/MyDrive/Colab_Notebooks/arcnn/outputs/\"+fname)\n",
        "  prog.update(count)\n",
        "print(\"\\nDone\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 2s 537ms/frames\n",
            "\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7lgEZIo_WyB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}